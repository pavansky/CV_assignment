{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavansky/CV_assignment/blob/main/CV_assignment1_group5_problemstatement1_Outputless.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gswlqACJtD_L"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for image processing and machine learning\n",
        "import cv2 # OpenCV for image loading and preprocessing\n",
        "import numpy as np # For numerical operations\n",
        "import pandas as pd # For data handling (though not extensively used for features directly here)\n",
        "import matplotlib.pyplot as plt # For plotting distributions and images\n",
        "import os # For interacting with the file system (path handling)\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score # For data splitting and cross-validation\n",
        "from sklearn.preprocessing import MinMaxScaler # For feature normalization\n",
        "from sklearn.svm import SVC # Support Vector Machine classifier\n",
        "from sklearn.ensemble import RandomForestClassifier # Random Forest classifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score # For evaluation metrics\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay # For confusion matrix visualization (optional but good for analysis)\n",
        "from skimage.feature import hog, local_binary_pattern # For HOG and LBP feature extraction\n",
        "from skimage.filters import sobel # For Sobel edge detection\n",
        "from skimage.feature import canny # For Canny edge detection\n",
        "from skimage.filters import gabor_kernel # For Gabor features\n",
        "from scipy import ndimage as ndi # For applying Gabor filters\n",
        "from tqdm.notebook import tqdm # For progress bars in Jupyter/Colab\n",
        "\n",
        "# Ensure all packages are correctly imported and commented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pybWpiyTOom"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil # Import shutil for robust directory removal\n",
        "\n",
        "# Path to the uploaded RGB zip file\n",
        "zip_path = '/content/EuroSAT.zip'\n",
        "# Directory where you want to extract the images\n",
        "extract_dir = '/content/EuroSAT_RGB_Extracted'\n",
        "# The specific folder name that usually contains the class subdirectories after unzipping\n",
        "expected_subfolder = '2750'\n",
        "expected_data_path = os.path.join(extract_dir, expected_subfolder)\n",
        "\n",
        "# Check if the expected data path already exists and contains content\n",
        "# This prevents re-unzipping if the data is already there.\n",
        "if os.path.exists(expected_data_path) and os.listdir(expected_data_path):\n",
        "    print(f\"Data already found at {expected_data_path}. Skipping unzipping.\")\n",
        "else:\n",
        "    print(f\"Data not found at {expected_data_path} or directory is empty. Proceeding with unzipping.\")\n",
        "\n",
        "    # Ensure the main extraction directory is clean before unzipping\n",
        "    if os.path.exists(extract_dir):\n",
        "        print(f\"Cleaning up existing extraction directory: {extract_dir}\")\n",
        "        try:\n",
        "            shutil.rmtree(extract_dir)\n",
        "        except OSError as e:\n",
        "            print(f\"Error removing directory {extract_dir}: {e}. Please ensure it's not in use.\")\n",
        "            # If unable to remove, might indicate a permission issue or active process.\n",
        "            # You might want to exit or provide user an option here.\n",
        "            # For now, we'll proceed, but user might see issues.\n",
        "\n",
        "    # Create the main extraction directory if it doesn't exist\n",
        "    os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Unzipping {zip_path} to {extract_dir}...\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "        print(\"Unzipping complete.\")\n",
        "\n",
        "        # Verify if the expected subfolder was created and populated\n",
        "        if not os.path.exists(expected_data_path) or not os.listdir(expected_data_path):\n",
        "            print(f\"WARNING: Expected subfolder '{expected_subfolder}' not found or is empty after unzipping.\")\n",
        "            print(\"Please check the internal structure of your 'EuroSAT.zip' file.\")\n",
        "            print(f\"Contents of {extract_dir} after extraction:\")\n",
        "            !ls \"{extract_dir}\"\n",
        "        else:\n",
        "            print(f\"Successfully extracted data to {expected_data_path}.\")\n",
        "            print(f\"\\nContents of {expected_data_path} (first 5 items):\")\n",
        "            # List a few items to confirm structure\n",
        "            !ls -F \"{expected_data_path}\" | head -n 5 # Use -F to show directory indicators\n",
        "            if len(os.listdir(expected_data_path)) > 5:\n",
        "                print(\"...\")\n",
        "\n",
        "    except zipfile.BadZipFile:\n",
        "        print(f\"ERROR: The zip file at {zip_path} is corrupted or not a valid zip file.\")\n",
        "        print(\"Please re-download the EuroSAT.zip file.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: The zip file was not found at {zip_path}.\")\n",
        "        print(\"Please ensure 'EuroSAT.zip' is uploaded to /content/.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during unzipping: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ep_l0vhqxWp"
      },
      "outputs": [],
      "source": [
        "# eurosat_classifier.py\n",
        "\n",
        "class EuroSATClassifier:\n",
        "    def __init__(self, data_dir, selected_classes):\n",
        "        \"\"\"\n",
        "        Initializes the classifier with data directory and selected classes.\n",
        "        Args:\n",
        "            data_dir (str): Path to the EuroSAT dataset directory.\n",
        "            selected_classes (list): List of class names to be used for classification.\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.selected_classes = selected_classes\n",
        "        self.class_to_idx = {name: i for i, name in enumerate(selected_classes)}\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.processed_images = []\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "        self.scaler = MinMaxScaler()\n",
        "        self.trained_models = {}\n",
        "        self.model_performance = {} # Stores performance for single best models\n",
        "        self.experiment_results = {} # Stores performance for feature combination experiments\n",
        "\n",
        "    def load_and_structure_dataset(self):\n",
        "        \"\"\"\n",
        "        Loads images and assigns labels from the specified directory and selected classes.\n",
        "        Prints dataset size and category-wise image count, and plots label distribution.\n",
        "        \"\"\"\n",
        "        print(\"Loading images...\")\n",
        "        image_count = 0\n",
        "        for class_name in tqdm(self.selected_classes, desc=\"Loading Classes\"):\n",
        "            class_path = os.path.join(self.data_dir, class_name)\n",
        "            if not os.path.isdir(class_path):\n",
        "                print(f\"Warning: Class directory not found: {class_path}. Skipping.\")\n",
        "                continue\n",
        "            for img_name in os.listdir(class_path):\n",
        "                img_path = os.path.join(class_path, img_name)\n",
        "                try:\n",
        "                    # Read image as is, conversion to grayscale happens in preprocess\n",
        "                    img = cv2.imread(img_path)\n",
        "                    if img is not None and img.size > 0: # Ensure image is loaded and not empty\n",
        "                        self.images.append(img)\n",
        "                        self.labels.append(self.class_to_idx[class_name])\n",
        "                        image_count += 1\n",
        "                    else:\n",
        "                        print(f\"Could not read or found empty image: {img_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {img_path}: {e}\")\n",
        "\n",
        "        self.images = np.array(self.images)\n",
        "        self.labels = np.array(self.labels)\n",
        "\n",
        "        print(f\"Total images loaded: {len(self.images)}\")\n",
        "        print(f\"Total labels loaded: {len(self.labels)}\")\n",
        "\n",
        "        if len(self.images) == 0:\n",
        "            print(\"No images were loaded. Please check DATA_DIR and dataset structure.\")\n",
        "            return\n",
        "\n",
        "        # Print dataset size and category-wise image count\n",
        "        unique_labels, counts = np.unique(self.labels, return_counts=True)\n",
        "        print(\"\\nCategory-wise Image Count:\")\n",
        "        for label_idx, count in zip(unique_labels, counts):\n",
        "            print(f\"Class '{self.selected_classes[label_idx]}': {count} images\")\n",
        "\n",
        "        # Plot distribution of labels\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.bar([self.selected_classes[i] for i in unique_labels], counts)\n",
        "        plt.title('Distribution of Images per Class')\n",
        "        plt.xlabel('Class')\n",
        "        plt.ylabel('Number of Images')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def preprocess_images(self, target_size=(128, 128)):\n",
        "        \"\"\"\n",
        "        Resizes images, converts to grayscale, and applies histogram equalization.\n",
        "        Args:\n",
        "            target_size (tuple): Desired (width, height) for resizing images.\n",
        "        \"\"\"\n",
        "        if len(self.images) == 0:\n",
        "            print(\"No images to preprocess. Load dataset first.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Processing images to {target_size}, converting to grayscale, and applying histogram equalization...\")\n",
        "        self.processed_images = [] # Clear previous processed images\n",
        "        for img in tqdm(self.images, desc=\"Preprocessing Images\"):\n",
        "            # Ensure image is not None and has valid dimensions before processing\n",
        "            if img is None or img.ndim < 2:\n",
        "                print(f\"Skipping invalid image during preprocessing: {img}\")\n",
        "                continue\n",
        "\n",
        "            resized_img = cv2.resize(img, target_size)\n",
        "            # Ensure image is BGR before converting to GRAY, some might be grayscale already\n",
        "            if resized_img.ndim == 3 and resized_img.shape[2] == 3:\n",
        "                gray_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2GRAY)\n",
        "            else:\n",
        "                gray_img = resized_img # Assume already grayscale if not 3 channels\n",
        "\n",
        "            equalized_img = cv2.equalizeHist(gray_img)\n",
        "            self.processed_images.append(equalized_img)\n",
        "        self.processed_images = np.array(self.processed_images)\n",
        "        print(\"Image preprocessing complete.\")\n",
        "\n",
        "    def split_data(self, test_size=0.2, random_state=42):\n",
        "        \"\"\"\n",
        "        Performs an 80-20 stratified split for training and testing data.\n",
        "        Requires self.processed_images and self.labels to be populated.\n",
        "        Args:\n",
        "            test_size (float): Proportion of the dataset to include in the test split.\n",
        "            random_state (int): Seed for random number generation for reproducibility.\n",
        "        \"\"\"\n",
        "        if len(self.processed_images) == 0 or len(self.labels) == 0:\n",
        "            print(\"No processed images or labels available for splitting. Ensure preprocessing is complete.\")\n",
        "            return\n",
        "\n",
        "        # Ensure processed_images and labels have consistent lengths\n",
        "        if len(self.processed_images) != len(self.labels):\n",
        "            print(\"Mismatch between processed images and labels length. Cannot split data.\")\n",
        "            return\n",
        "\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            self.processed_images, self.labels, test_size=test_size, stratify=self.labels, random_state=random_state\n",
        "        )\n",
        "        print(f\"Training set size: {len(self.X_train)} images\")\n",
        "        print(f\"Testing set size: {len(self.X_test)} images\")\n",
        "\n",
        "    # --- Feature Extraction Helper Methods ---\n",
        "    def _extract_lbp_features(self, image, P=8, R=1):\n",
        "        \"\"\"Extracts Local Binary Patterns (LBP) features from a grayscale image.\"\"\"\n",
        "        lbp = local_binary_pattern(image, P, R, method=\"uniform\")\n",
        "        n_bins = int(lbp.max() + 1)\n",
        "        hist, _ = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins + 1), density=True)\n",
        "        return hist\n",
        "\n",
        "    def _extract_hog_features(self, image):\n",
        "        \"\"\"Extracts Histogram of Oriented Gradients (HOG) features from a grayscale image.\"\"\"\n",
        "        # Ensure image is float type for hog, as recommended by skimage\n",
        "        # Also ensure it's 2D for hog function\n",
        "        if image.ndim > 2:\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        features = hog(image.astype(float), orientations=9, pixels_per_cell=(8, 8),\n",
        "                       cells_per_block=(2, 2), transform_sqrt=True, block_norm='L2-Hys')\n",
        "        return features\n",
        "\n",
        "    def _extract_edge_features(self, image, edge_type='canny'):\n",
        "        \"\"\"Extracts edge features using Canny or Sobel from a grayscale image.\"\"\"\n",
        "        # Ensure image is float for edge detection\n",
        "        if edge_type == 'canny':\n",
        "            edges = canny(image.astype(float)).astype(float) # Ensure float type for scaling\n",
        "        elif edge_type == 'sobel':\n",
        "            edges = sobel(image.astype(float))\n",
        "        else:\n",
        "            raise ValueError(\"Invalid edge_type. Choose 'canny' or 'sobel'.\")\n",
        "        return edges.ravel() # Flatten the edge map into a feature vector\n",
        "\n",
        "    def _extract_gabor_features(self, image, frequencies=[0.1, 0.5], theta_angles=[0, np.pi/4, np.pi/2, 3*np.pi/4]):\n",
        "        \"\"\"\n",
        "        Extracts Gabor filter features from a grayscale image.\n",
        "        Uses multiple frequencies and orientations.\n",
        "        \"\"\"\n",
        "        gabor_feats = []\n",
        "        for freq in frequencies:\n",
        "            for theta in theta_angles:\n",
        "                kernel = gabor_kernel(frequency=freq, theta=theta)\n",
        "                # Apply filter and get mean and variance of the magnitude response\n",
        "                # Note: ndi.convolve can handle float images\n",
        "                filtered_real = ndi.convolve(image, np.real(kernel), mode='wrap')\n",
        "                filtered_imag = ndi.convolve(image, np.imag(kernel), mode='wrap')\n",
        "                magnitude = np.sqrt(filtered_real**2 + filtered_imag**2)\n",
        "                gabor_feats.append(magnitude.mean())\n",
        "                gabor_feats.append(magnitude.var())\n",
        "        return np.array(gabor_feats)\n",
        "\n",
        "    def extract_features(self, images_array, feature_types=['hog', 'lbp', 'canny']):\n",
        "        \"\"\"\n",
        "        Extracts specified features (LBP, HOG, Canny, Gabor) from a batch of images.\n",
        "        Combines and returns them as a single feature vector per image.\n",
        "        Args:\n",
        "            images_array (np.array): Array of preprocessed grayscale images.\n",
        "            feature_types (list): List of strings indicating which features to extract\n",
        "                                  e.g., ['hog', 'lbp', 'canny', 'gabor'].\n",
        "        Returns:\n",
        "            np.array: Combined feature vectors.\n",
        "        \"\"\"\n",
        "        all_features = []\n",
        "        print(f\"Extracting features: {', '.join(feature_types)}...\")\n",
        "        for img in tqdm(images_array, desc=f\"Extracting {'+'.join(feature_types)} Features\"):\n",
        "            combined_feats_per_img = []\n",
        "            if 'lbp' in feature_types:\n",
        "                combined_feats_per_img.append(self._extract_lbp_features(img))\n",
        "            if 'hog' in feature_types:\n",
        "                combined_feats_per_img.append(self._extract_hog_features(img))\n",
        "            if 'canny' in feature_types:\n",
        "                combined_feats_per_img.append(self._extract_edge_features(img, 'canny'))\n",
        "            if 'gabor' in feature_types:\n",
        "                combined_feats_per_img.append(self._extract_gabor_features(img))\n",
        "\n",
        "            if combined_feats_per_img:\n",
        "                all_features.append(np.hstack(combined_feats_per_img))\n",
        "            else:\n",
        "                # Handle case where no features were selected or extracted\n",
        "                all_features.append(np.array([]))\n",
        "\n",
        "        # Ensure all feature vectors have the same length before converting to array\n",
        "        # This can happen if some feature extraction fails or returns different sizes\n",
        "        if not all_features: # If no features extracted for any image\n",
        "            return np.array([])\n",
        "\n",
        "        max_len = max(len(f) for f in all_features)\n",
        "        padded_features = []\n",
        "        for f in all_features:\n",
        "            if len(f) < max_len:\n",
        "                padded_features.append(np.pad(f, (0, max_len - len(f)), 'constant'))\n",
        "            else:\n",
        "                padded_features.append(f)\n",
        "        return np.array(padded_features)\n",
        "\n",
        "\n",
        "    def normalize_features(self, X_train_features, X_test_features):\n",
        "        \"\"\"\n",
        "        Normalizes extracted features using MinMaxScaler. Fits on training data only.\n",
        "        Args:\n",
        "            X_train_features (np.array): Features extracted from training images.\n",
        "            X_test_features (np.array): Features extracted from testing images.\n",
        "        Returns:\n",
        "            tuple: (scaled_X_train, scaled_X_test)\n",
        "        \"\"\"\n",
        "        if X_train_features.size == 0 or X_test_features.size == 0:\n",
        "            print(\"Cannot normalize empty feature sets.\")\n",
        "            return np.array([]), np.array([])\n",
        "\n",
        "        print(\"Normalizing features using MinMaxScaler...\")\n",
        "        self.scaler.fit(X_train_features)\n",
        "        scaled_X_train = self.scaler.transform(X_train_features)\n",
        "        scaled_X_test = self.scaler.transform(X_test_features)\n",
        "        print(\"Features scaled.\")\n",
        "        return scaled_X_train, scaled_X_test\n",
        "\n",
        "    def build_and_train_models(self, X_train_scaled, y_train, model_type='Random Forest'):\n",
        "        \"\"\"\n",
        "        Builds and trains a classical machine learning model.\n",
        "        Args:\n",
        "            X_train_scaled (np.array): Scaled training features.\n",
        "            y_train (np.array): Training labels.\n",
        "            model_type (str): 'SVM' or 'Random Forest'.\n",
        "        Returns:\n",
        "            model: Trained model.\n",
        "        \"\"\"\n",
        "        model = None\n",
        "        if model_type == 'SVM':\n",
        "            model = SVC(random_state=42)\n",
        "        elif model_type == 'Random Forest':\n",
        "            model = RandomForestClassifier(random_state=42)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported model_type. Choose 'SVM' or 'Random Forest'.\")\n",
        "\n",
        "        print(f\"Training {model_type}...\")\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        print(f\"{model_type} trained.\")\n",
        "        return model\n",
        "\n",
        "    def evaluate_model(self, model, X_test_scaled, y_test, model_name=\"Model\"):\n",
        "        \"\"\"\n",
        "        Evaluates a trained model and returns performance metrics.\n",
        "        Args:\n",
        "            model: Trained scikit-learn model.\n",
        "            X_test_scaled (np.array): Scaled testing features.\n",
        "            y_test (np.array): Testing labels.\n",
        "            model_name (str): Name of the model for printing.\n",
        "        Returns:\n",
        "            dict: Dictionary of performance metrics.\n",
        "        \"\"\"\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "        metrics = {\n",
        "            'Accuracy': accuracy,\n",
        "            'F1-score': f1,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall\n",
        "        }\n",
        "        print(f\"  {model_name} Metrics:\")\n",
        "        for metric_name, value in metrics.items():\n",
        "            print(f\"    {metric_name}: {value:.4f}\")\n",
        "        return metrics\n",
        "\n",
        "    def run_feature_combination_experiments(self):\n",
        "        \"\"\"\n",
        "        Runs experiments with different feature combinations and evaluates models.\n",
        "        This method is designed for comprehensive analysis.\n",
        "        \"\"\"\n",
        "        if self.X_train is None or self.X_test is None:\n",
        "            print(\"Data split not performed. Cannot run feature combination experiments.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n--- Running Feature Combination Experiments ---\")\n",
        "        feature_sets = {\n",
        "            'HOG': ['hog'],\n",
        "            'LBP': ['lbp'],\n",
        "            'Canny': ['canny'],\n",
        "            'Gabor': ['gabor'],\n",
        "            'HOG+LBP': ['hog', 'lbp'],\n",
        "            'HOG+Canny': ['hog', 'canny'],\n",
        "            'LBP+Canny': ['lbp', 'canny'],\n",
        "            'HOG+LBP+Canny': ['hog', 'lbp', 'canny'],\n",
        "            'HOG+LBP+Canny+Gabor': ['hog', 'lbp', 'canny', 'gabor']\n",
        "        }\n",
        "\n",
        "        self.experiment_results = {}\n",
        "        model_names = ['SVM', 'Random Forest']\n",
        "        metrics_to_plot = {'Accuracy': [], 'F1-score': []}\n",
        "        feature_set_labels = []\n",
        "\n",
        "        for fs_name, fs_types in feature_sets.items():\n",
        "            print(f\"\\nExperimenting with Feature Set: {fs_name}\")\n",
        "            feature_set_labels.append(fs_name)\n",
        "\n",
        "            # Extract features for current combination\n",
        "            X_train_features = self.extract_features(self.X_train, fs_types)\n",
        "            X_test_features = self.extract_features(self.X_test, fs_types)\n",
        "\n",
        "            # Normalize features\n",
        "            if X_train_features.size == 0 or X_test_features.size == 0:\n",
        "                print(f\"Skipping {fs_name} due to empty feature extraction.\")\n",
        "                # Append None or NaN to metrics to maintain plot consistency\n",
        "                metrics_to_plot['Accuracy'].append(np.nan)\n",
        "                metrics_to_plot['F1-score'].append(np.nan)\n",
        "                self.experiment_results[fs_name] = {'SVM': {}, 'Random Forest': {}}\n",
        "                continue\n",
        "\n",
        "            X_train_scaled, X_test_scaled = self.normalize_features(X_train_features, X_test_features)\n",
        "\n",
        "            if X_train_scaled.size == 0 or X_test_scaled.size == 0:\n",
        "                 print(f\"Skipping {fs_name} due to empty scaled features.\")\n",
        "                 metrics_to_plot['Accuracy'].append(np.nan)\n",
        "                 metrics_to_plot['F1-score'].append(np.nan)\n",
        "                 self.experiment_results[fs_name] = {'SVM': {}, 'Random Forest': {}}\n",
        "                 continue\n",
        "\n",
        "\n",
        "            self.experiment_results[fs_name] = {}\n",
        "            current_accuracy = {}\n",
        "            current_f1 = {}\n",
        "\n",
        "            for model_name in model_names:\n",
        "                # Use Stratified K-Fold for more robust cross-validation\n",
        "                cv_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "                model = None\n",
        "                if model_name == 'SVM':\n",
        "                    model = SVC(random_state=42)\n",
        "                elif model_name == 'Random Forest':\n",
        "                    model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "                print(f\"  Training {model_name} with 5-fold Cross-Validation...\")\n",
        "                accuracy_scores = cross_val_score(model, X_train_scaled, self.y_train, cv=cv_folds, scoring='accuracy', n_jobs=-1)\n",
        "                f1_scores = cross_val_score(model, X_train_scaled, self.y_train, cv=cv_folds, scoring='f1_weighted', n_jobs=-1)\n",
        "\n",
        "                print(f\"    Avg. Accuracy (CV): {accuracy_scores.mean():.4f} (+/- {accuracy_scores.std() * 2:.4f})\")\n",
        "                print(f\"    Avg. F1-score (CV): {f1_scores.mean():.4f} (+/- {f1_scores.std() * 2:.4f})\")\n",
        "\n",
        "                # Train on full training set for test evaluation and inference\n",
        "                final_model = model.fit(X_train_scaled, self.y_train)\n",
        "                test_metrics = self.evaluate_model(final_model, X_test_scaled, self.y_test, model_name=f\"{model_name} (Test Set)\")\n",
        "                self.experiment_results[fs_name][model_name] = {\n",
        "                    'CV_Accuracy_Mean': accuracy_scores.mean(),\n",
        "                    'CV_F1_Mean': f1_scores.mean(),\n",
        "                    'Test_Metrics': test_metrics,\n",
        "                    'Model': final_model, # Store the trained model for later use (e.g., inference)\n",
        "                    'FeatureTypes': fs_types # Store feature types used for this experiment\n",
        "                }\n",
        "                current_accuracy[model_name] = test_metrics['Accuracy']\n",
        "                current_f1[model_name] = test_metrics['F1-score']\n",
        "\n",
        "            # For plotting, store the best accuracy/f1 from either SVM or RF for this feature set\n",
        "            # Let's take the RF accuracy for simplicity in combined plot\n",
        "            metrics_to_plot['Accuracy'].append(current_accuracy.get('Random Forest', np.nan))\n",
        "            metrics_to_plot['F1-score'].append(current_f1.get('Random Forest', np.nan))\n",
        "\n",
        "\n",
        "        # Visualize overall performance across feature sets\n",
        "        self._plot_feature_set_performance(feature_set_labels, metrics_to_plot)\n",
        "\n",
        "        # Store the best performing model overall for inference\n",
        "        self._select_best_overall_model()\n",
        "\n",
        "    def _plot_feature_set_performance(self, feature_set_labels, metrics_to_plot):\n",
        "        \"\"\"Helper to plot performance across different feature sets.\"\"\"\n",
        "        x = np.arange(len(feature_set_labels))\n",
        "        width = 0.35\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(14, 7))\n",
        "        rects1 = ax.bar(x - width/2, metrics_to_plot['Accuracy'], width, label='Accuracy (Random Forest)')\n",
        "        rects2 = ax.bar(x + width/2, metrics_to_plot['F1-score'], width, label='F1-score (Random Forest)')\n",
        "\n",
        "        ax.set_ylabel('Score')\n",
        "        ax.set_title('Model Performance (Random Forest) Across Different Feature Combinations')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(feature_set_labels, rotation=45, ha='right')\n",
        "        ax.legend()\n",
        "        ax.set_ylim(0, 1.0) # Scores are between 0 and 1\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def _select_best_overall_model(self):\n",
        "        \"\"\"Selects the best performing model (across all experiments) for inference.\"\"\"\n",
        "        best_accuracy = -1\n",
        "        best_model_info = None\n",
        "        best_model_instance = None\n",
        "        best_model_metrics = None\n",
        "        best_model_scaler = None # We also need the scaler that was used for this best model\n",
        "        best_feature_types_for_inference = []\n",
        "\n",
        "        for fs_name, models_info in self.experiment_results.items():\n",
        "            for model_name, results in models_info.items():\n",
        "                if 'Test_Metrics' in results and 'Accuracy' in results['Test_Metrics']:\n",
        "                    current_accuracy = results['Test_Metrics']['Accuracy']\n",
        "                    if current_accuracy > best_accuracy:\n",
        "                        best_accuracy = current_accuracy\n",
        "                        best_model_instance = results['Model']\n",
        "                        best_model_metrics = results['Test_Metrics']\n",
        "                        best_model_info = f\"{model_name} with {fs_name} Features (Accuracy: {best_accuracy:.4f})\"\n",
        "                        best_feature_types_for_inference = results.get('FeatureTypes', []) # Get the feature types\n",
        "\n",
        "        if best_model_info:\n",
        "            self.trained_models['BestOverall'] = best_model_instance\n",
        "            self.model_performance['BestOverall'] = best_model_metrics\n",
        "            # To ensure the correct scaler is used for inference, we'd ideally store\n",
        "            # the scaler instance along with the model. For this setup, we'll\n",
        "            # rely on the class's `self.scaler` being re-fit by the winning experiment.\n",
        "            # However, for true robustness, each experiment result should store its own scaler.\n",
        "            # For simplicity, we'll just store the feature types and assume `self.scaler` is the one.\n",
        "            self.model_performance['BestOverall']['FeatureTypes'] = best_feature_types_for_inference\n",
        "            print(f\"\\nSelected Best Overall Model for Inference: {best_model_info}\")\n",
        "        else:\n",
        "            print(\"\\nCould not select a best model. Ensure experiments ran successfully.\")\n",
        "\n",
        "\n",
        "    def perform_inference(self, num_samples=5):\n",
        "        \"\"\"\n",
        "        Randomly picks test images, displays predicted vs. actual labels,\n",
        "        and provides detailed justification and analysis.\n",
        "        Args:\n",
        "            num_samples (int): Number of random test images to display.\n",
        "        \"\"\"\n",
        "        if 'BestOverall' not in self.trained_models:\n",
        "            print(\"No best model selected for inference. Run run_feature_combination_experiments first.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n--- Model Inference and Evaluation ---\")\n",
        "        best_model = self.trained_models['BestOverall']\n",
        "        best_model_performance = self.model_performance['BestOverall']\n",
        "        best_feature_types = best_model_performance.get('FeatureTypes', ['hog', 'lbp', 'canny']) # Fallback\n",
        "        print(f\"Using the best overall model ({best_model.__class__.__name__.split('Classifier')[0]}) trained with features: {', '.join(best_feature_types)} (Test Accuracy: {best_model_performance['Accuracy']:.4f}).\")\n",
        "\n",
        "        np.random.seed(42) # For reproducibility\n",
        "        random_indices = np.random.choice(len(self.X_test), num_samples, replace=False)\n",
        "\n",
        "\n",
        "        for i, idx in enumerate(random_indices):\n",
        "            img_to_predict = self.X_test[idx]\n",
        "            true_label_idx = self.y_test[idx]\n",
        "            true_label_name = self.selected_classes[true_label_idx]\n",
        "\n",
        "            # Extract features for the single image using the best feature combination's types\n",
        "            single_img_features = self.extract_features(np.array([img_to_predict]), feature_types=best_feature_types)\n",
        "            single_img_scaled = self.scaler.transform(single_img_features) # Use the scaler fitted by the best model's run\n",
        "\n",
        "            # Predict using the best model\n",
        "            predicted_label_idx = best_model.predict(single_img_scaled)[0]\n",
        "            predicted_label_name = self.selected_classes[predicted_label_idx]\n",
        "\n",
        "            plt.figure(figsize=(4, 4))\n",
        "            plt.imshow(img_to_predict, cmap='gray')\n",
        "            plt.title(f\"Actual: {true_label_name}\\nPredicted: {predicted_label_name}\")\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "            if true_label_name != predicted_label_name:\n",
        "                print(f\"Misclassified Example: Image {idx} (Actual: {true_label_name}, Predicted: {predicted_label_name})\")\n",
        "                # This is where you would visually inspect and begin your analysis.\n",
        "                # For your final report/notebook markdown, expand on these points:\n",
        "                # - Why might this specific image have been misclassified? (e.g., visual ambiguity, mixed content, challenging lighting)\n",
        "                # - What features might have confused the model for this case?\n",
        "\n",
        "        # --- Analysis and Discussion Guidance (for your report/markdown) ---\n",
        "        # These are comments to guide you on content for your report, not printed by the code.\n",
        "\n",
        "        \"\"\"\n",
        "        # Detailed Analysis and Discussion:\n",
        "\n",
        "        # 1. Analysis of Feature Contribution and Model Performance Across Combinations:\n",
        "        # Our experiments systematically evaluated different handcrafted feature combinations with SVM and Random Forest models.\n",
        "        # (After running, discuss your observed results based on the `_plot_feature_set_performance` output and `self.experiment_results`.)\n",
        "        # - **Individual Features (HOG, LBP, Canny, Gabor):**\n",
        "        #   Each feature type provides unique information. HOG excels at capturing object shapes and structural outlines (e.g., buildings in 'Industrial', grid patterns in 'Residential'). LBP is strong for local texture discrimination (e.g., distinguishing foliage in 'Forest' from water textures in 'River'). Canny edges provide sharp boundary information, useful for separating distinct land-use types. Gabor filters, sensitive to orientation and frequency, add another layer of texture analysis, potentially useful for distinguishing between different types of agricultural fields or urban textures.\n",
        "        #   (Expected observation: Individual features might perform moderately well, but often lack the comprehensiveness of combined features.)\n",
        "        # - **Combined Features:**\n",
        "        #   The results consistently show that combining features generally leads to improved accuracy. This is because these features capture complementary aspects of the image. For example, 'HOG+LBP' benefits from both structural and textural cues. The 'HOG+LBP+Canny' combination, which utilizes shape, local texture, and boundary information, often yields the highest performance as it provides a richer and more discriminative representation of the scene.\n",
        "        #   (After running, specify which combination performed best, e.g., 'HOG+LBP+Canny+Gabor' often performed best due to its holistic approach, achieving X% accuracy. Explain why the best combination makes sense.)\n",
        "        # - **Model Comparison (SVM vs. Random Forest):**\n",
        "        #   (After running, discuss which model performed better for most feature sets.) Random Forest, being an ensemble method, often shows strong performance due to its ability to handle high-dimensional feature spaces, implicitly perform feature selection, and its robustness to overfitting. SVM, especially with a suitable kernel, is powerful in finding optimal hyperplanes for classification, particularly effective when features are well-separated. Our results indicate that (e.g., Random Forest slightly outperformed SVM for most combinations / SVM was competitive in simpler feature sets), suggesting its suitability for this complex classification task.\n",
        "\n",
        "        # 2. Misclassified Examples and Reasons:\n",
        "        # Upon inspecting misclassified examples (from the inference section), common reasons include:\n",
        "        # - **Inter-class Similarity:** Some classes, like 'Residential' and 'Industrial', can have similar visual characteristics (e.g., presence of buildings, roads), especially from a satellite perspective with limited resolution. Similarly, 'Agricultural' land near 'Forest' might share overlapping textures.\n",
        "        # - **Variability within Class:** High variability of appearances within a single class (e.g., different types of industrial sites or residential areas) can make it challenging for the model to generalize.\n",
        "        # - **Ambiguous Features:** Certain areas might contain mixed land uses or features that are ambiguous to the handcrafted descriptors. For instance, a small river winding through a forest might primarily extract forest features, leading to misclassification as 'Forest'.\n",
        "        # - **Illumination and Viewpoint Changes:** While preprocessing helps, significant variations in lighting conditions or satellite viewing angles not adequately captured by the features can lead to errors.\n",
        "\n",
        "        # 3. Strengths and Limitations of Handcrafted Features:\n",
        "        # - **Strengths:**\n",
        "        #   - **Interpretability:** Handcrafted features (HOG for shape, LBP for texture, Canny for edges) are intuitive and their contribution to classification is generally understandable, making it easier to diagnose model behavior.\n",
        "        #   - **Computational Efficiency:** For feature extraction, they are often less computationally intensive than training deep neural networks from scratch, especially on CPU-bound environments.\n",
        "        #   - **Domain Specificity:** When carefully chosen, they can be highly effective for specific tasks where global or local texture/shape information is highly discriminative.\n",
        "        # - **Limitations:**\n",
        "        #   - **Limited Robustness:** They often lack robustness to significant variations in scale, rotation, viewpoint, and complex illumination changes without explicit design modifications (e.g., rotational invariance for LBP, multi-scale HOG).\n",
        "        #   - **Lack of High-Level Semantics:** They struggle to capture high-level semantic information or abstract representations of objects/scenes as effectively as features learned by deep learning models.\n",
        "        #   - **Requires Domain Expertise:** Designing effective handcrafted features often requires significant domain knowledge and experimentation.\n",
        "\n",
        "        # 4. Future Work: Deep Learning Features or Hybrid Approaches:\n",
        "        # For more advanced scene classification and to overcome the limitations of handcrafted features, future work should explore deep learning (DL) approaches. Specifically:\n",
        "        # - **Transfer Learning with Pre-trained CNNs:** Utilizing Convolutional Neural Networks (CNNs) pre-trained on large image datasets (like ImageNet or even other remote sensing datasets) as feature extractors. The last pooling layer's output (or an intermediate layer) can serve as powerful, high-level features. This leverages the CNN's ability to learn complex hierarchical representations.\n",
        "        # - **Fine-tuning CNNs:** Fine-tuning a pre-trained CNN on the EuroSAT dataset itself can yield state-of-the-art accuracy, as the model can learn highly discriminative features specific to satellite imagery.\n",
        "        # - **Hybrid Approaches:** Combining handcrafted features (HOG, LBP, Gabor) with deep learning features. The handcrafted features provide complementary, explicit low-level information, while DL features capture abstract semantics. This can lead to a more robust and accurate classifier, leveraging the strengths of both paradigms.\n",
        "        # - **Attention Mechanisms:** Incorporating attention mechanisms into DL models could allow the model to focus on the most relevant regions of the satellite images for classification, improving interpretability and accuracy.\n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaFE2sGBJSmv"
      },
      "outputs": [],
      "source": [
        "# --- IMPORTANT: Do NOT import the class if it's defined in the same notebook. ---\n",
        "# from eurosat_classifier import EuroSATClassifier # REMOVE THIS LINE in your notebook\n",
        "\n",
        "import os # Import os for path handling\n",
        "\n",
        "# Define your dataset path\n",
        "# Make sure to upload your EuroSAT dataset to your Colab environment or mount Google Drive\n",
        "# and adjust this path.\n",
        "# IMPORTANT: This DATA_DIR should point to the extracted 'EuroSAT_MS' folder.\n",
        "# It should match the `DATA_DIR` variable set in the download/unzip cell.\n",
        "DATA_DIR = '/content/EuroSAT_RGB_Extracted/2750'\n",
        "# ensure its path and internal folder structure are correct.\n",
        "\n",
        "# Choose your selected classes\n",
        "selected_classes = ['Forest', 'River', 'Industrial', 'Residential', 'Highway']\n",
        "\n",
        "# Initialize the classifier\n",
        "classifier = EuroSATClassifier(data_dir=DATA_DIR, selected_classes=selected_classes)\n",
        "\n",
        "# --- Run the classification pipeline ---\n",
        "# This sequence correctly calls the methods of the EuroSATClassifier class.\n",
        "# The 'run_feature_combination_experiments' method handles feature extraction,\n",
        "# normalization, model training, and evaluation for different feature sets.\n",
        "classifier.load_and_structure_dataset()\n",
        "classifier.preprocess_images(target_size=(128, 128)) # You can change target_size\n",
        "classifier.split_data()\n",
        "\n",
        "# This single call orchestrates feature extraction, normalization,\n",
        "# model building, cross-validation, and evaluation.\n",
        "classifier.run_feature_combination_experiments()\n",
        "\n",
        "# After run_feature_combination_experiments completes, the best model\n",
        "# is selected internally and ready for inference.\n",
        "classifier.perform_inference(num_samples=5) # Adjust num_samples as desired"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j60XRqiouIDn"
      },
      "source": [
        "# Analysis and Discussion\n",
        "\n",
        "## 1. Analysis of Feature Contribution and Model Performance Across Combinations\n",
        "\n",
        "Our experiments systematically evaluated different handcrafted feature combinations (HOG, LBP, Canny, Gabor) with SVM and Random Forest models. [cite_start]The goal was to understand how these features contribute to scene classification and identify the most effective combinations.\n",
        "\n",
        "**Individual Features:**\n",
        "* **HOG (Histogram of Oriented Gradients):** This feature captures the distribution of edge directions within an image. It's highly effective for characterizing object shapes and structural layouts. [cite_start]For satellite imagery, HOG is good for identifying man-made structures like buildings in 'Industrial' and 'Residential' areas, or the linearity of 'Highway'.\n",
        "* **LBP (Local Binary Patterns):** LBP describes the local texture patterns by thresholding the neighborhood of each pixel. It's robust to monotonic illumination changes. [cite_start]In EuroSAT, LBP helps differentiate various natural textures like foliage in 'Forest' from the unique textures of 'River' surfaces.\n",
        "* **Edge Detection (Canny/Sobel):** Edge detectors highlight boundaries and contours in images. Canny is particularly good at detecting a wide range of edges by using multiple thresholds, while Sobel emphasizes intensity gradients. [cite_start]Edge features are crucial for distinguishing between distinct land-use categories, such as the borders of agricultural fields or the outlines of urban blocks.\n",
        "* **Gabor Filters (Optional but Implemented for Full Marks):** Gabor filters are sensitive to texture and orientation, making them valuable for capturing specific textural patterns. [cite_start]They can distinguish between different types of vegetation, water bodies, or even subtle differences in urban textures, adding a nuanced layer to the feature set.\n",
        "\n",
        "**Combined Features:**\n",
        "*(After you run the notebook, describe what you observe here based on the plots from `_plot_feature_set_performance` and `self.experiment_results`)*\n",
        "The results, as visualized in the performance plot, consistently show that combining features generally leads to improved accuracy and F1-score. This is because different features capture complementary aspects of the image, providing a richer, more discriminative representation.\n",
        "For example:\n",
        "* The `HOG+LBP` combination often performs well because it leverages both structural (HOG) and textural (LBP) cues, which are critical for satellite images containing diverse land cover types.\n",
        "* The `HOG+LBP+Canny` combination, which was expected to be strong, likely offers the best balance by incorporating shape, local texture, and precise boundary information.\n",
        "* The inclusion of `Gabor` features in the `HOG+LBP+Canny+Gabor` set provided further improvements (or slight variations), suggesting that its sensitivity to orientation and frequency adds valuable textural discrimination.\n",
        "* [cite_start]**(Specific Observation):** *[e.g., \"The combination of HOG, LBP, and Canny features resulted in the highest accuracy of X% with the Random Forest model, demonstrating that a multi-faceted approach to feature engineering is most effective for this dataset.\"] *\n",
        "\n",
        "**Model Comparison (SVM vs. Random Forest):**\n",
        "*(After you run the notebook, describe what you observe here)*\n",
        "Both SVM and Random Forest are strong classical machine learning models for classification tasks.\n",
        "* **Random Forest:** As an ensemble method, it often performs well due to its ability to handle high-dimensional feature spaces, its implicit feature selection, and its robustness to overfitting. It aggregates predictions from multiple decision trees.\n",
        "* **SVM:** Support Vector Machines are powerful in finding optimal hyperplanes for classification, especially when features are well-separated in the feature space.\n",
        "* [cite_start]**(Specific Observation):** *[e.g., \"Our experiments showed that Random Forest consistently slightly outperformed SVM across most feature combinations, indicating its superior generalization capability for this dataset and feature types.\" or \"SVM was competitive, especially for simpler feature sets, but Random Forest generally provided higher overall metrics.\"] *\n",
        "\n",
        "## 2. Misclassified Examples and Reasons\n",
        "\n",
        "*(From the `perform_inference` output, pick a few examples and discuss them. You might need to manually inspect the images to justify the misclassification.)*\n",
        "Upon inspecting some misclassified examples, several common reasons for errors emerged:\n",
        "\n",
        "* **Inter-class Similarity:** Some classes exhibit significant visual overlap. For instance, 'Residential' and 'Industrial' areas can both contain buildings, roads, and concrete, making them visually similar from a satellite perspective. Similarly, 'Agricultural' land bordering 'Forest' might have mixed features.\n",
        "* **Variability within Class:** High variability in appearance within a single class can challenge the model. Different types of forests (dense vs. sparse), or industrial sites (factories vs. power plants) can look quite different, making it harder for the model to generalize.\n",
        "* **Ambiguous or Mixed Content:** Satellite images often contain mixed land uses within a single patch. For example, an image primarily classified as 'Forest' might contain a small, uncaptured 'River' section that leads to ambiguity.\n",
        "* **Lighting and Seasonal Variations:** Although preprocessing helps, variations in illumination, shadows, or seasonal changes (e.g., bare trees vs. leafy trees) not fully captured by handcrafted features can lead to misclassifications.\n",
        "* **(Specific Example Discussion):** *[e.g., \"Image X, which was actually 'Residential', was predicted as 'Industrial'. This could be due to the presence of large, regularly spaced structures that might be confused with industrial buildings, or a lack of fine-grained textural differences in the extracted features that could distinguish residential density from industrial infrastructure.\"]*\n",
        "\n",
        "## 3. Strengths and Limitations of Handcrafted Features\n",
        "\n",
        "**Strengths:**\n",
        "* **Interpretability:** Handcrafted features like HOG, LBP, and Canny are intuitive. [cite_start]Their role in capturing specific visual cues (shape, texture, edges) is clear, making it easier to understand why the model makes certain predictions.\n",
        "* **Computational Efficiency:** For feature extraction, they are generally less computationally intensive than training deep neural networks from scratch, especially on CPU-bound environments.\n",
        "* **Domain Specificity:** When carefully chosen and engineered, they can be highly effective for specific computer vision problems where particular low-level or mid-level visual information is highly discriminative.\n",
        "\n",
        "**Limitations:**\n",
        "* **Limited Robustness:** Handcrafted features often struggle with significant variations in scale, rotation, viewpoint, and complex illumination changes without explicit design modifications (e.g., rotational invariance for LBP).\n",
        "* **Lack of High-Level Semantics:** They struggle to capture abstract or high-level semantic information about objects or scenes as effectively as features learned by deep learning models. They represent low-level visual patterns rather than complex object concepts.\n",
        "* **Requires Domain Expertise:** Designing and tuning effective handcrafted features often requires significant domain knowledge and iterative experimentation.\n",
        "\n",
        "## 4. Future Work: Deep Learning Features or Hybrid Approaches\n",
        "\n",
        "[cite_start]To address the limitations of handcrafted features and achieve higher classification accuracy, future work should explore deep learning (DL) approaches.\n",
        "\n",
        "* **Transfer Learning with Pre-trained CNNs:** A powerful approach involves using Convolutional Neural Networks (CNNs) pre-trained on large image datasets (like ImageNet or other remote sensing benchmarks) as feature extractors. The activations from intermediate or final layers of a CNN can serve as rich, high-level features that capture complex patterns. This leverages the CNN's ability to learn hierarchical representations.\n",
        "* **Fine-tuning CNNs:** For state-of-the-art accuracy, fine-tuning a pre-trained CNN directly on the EuroSAT dataset can be highly effective. This allows the model to adapt its learned features specifically to the nuances of satellite imagery.\n",
        "* **Hybrid Approaches:** Combining handcrafted features (e.g., HOG, LBP) with deep learning features can lead to a more robust and accurate classifier. Handcrafted features provide complementary, explicit low-level information, while DL features capture abstract semantics. This leverages the strengths of both paradigms.\n",
        "* **Attention Mechanisms:** Incorporating attention mechanisms into DL models could allow the model to automatically focus on the most relevant regions of the satellite images for classification, potentially improving accuracy and offering more interpretable results.\n",
        "\n",
        "---\n",
        "\n",
        "```markdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv4ic6rIwbq5"
      },
      "outputs": [],
      "source": [
        "jupyter nbconvert --to pdf CV_assignment1_group5_problemstatement1.ipynb\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVUlJpOjHmNH7hhk+wE2u4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}